{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gan.ipynb","provenance":[],"authorship_tag":"ABX9TyMMGBLc7SsxaG6V4ytpNIWZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CJWLEqXFekHx"},"source":["import os\n","import numpy as np\n","import cv2\n","from glob import glob\n","from matplotlib import pyplot\n","from sklearn.utils import shuffle\n","import tensorflow as tf\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","IMG_H = 64\n","IMG_W = 64\n","IMG_C = 3  ## Change this to 1 for grayscale.\n","w_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","def load_image(image_path):\n","    img = tf.io.read_file(image_path)\n","    img = tf.io.decode_jpeg(img)\n","    img = tf.image.resize_with_crop_or_pad(img, IMG_H, IMG_W)\n","    img = tf.cast(img, tf.float32)\n","    img = (img - 127.5) / 127.5\n","    return img\n","\n","def tf_dataset(images_path, batch_size):\n","    dataset = tf.data.Dataset.from_tensor_slices(images_path)\n","    dataset = dataset.shuffle(buffer_size=10240)\n","    dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","    return dataset\n","\n","def deconv_block(inputs, num_filters, kernel_size, strides, bn=True):\n","    x = Conv2DTranspose(\n","        filters=num_filters,\n","        kernel_size=kernel_size,\n","        kernel_initializer=w_init,\n","        padding=\"same\",\n","        strides=strides,\n","        use_bias=False\n","        )(inputs)\n","\n","    if bn:\n","        x = BatchNormalization()(x)\n","        x = LeakyReLU(alpha=0.2)(x)\n","    return x\n","\n","\n","def conv_block(inputs, num_filters, kernel_size, padding=\"same\", strides=2, activation=True):\n","    x = Conv2D(\n","        filters=num_filters,\n","        kernel_size=kernel_size,\n","        kernel_initializer=w_init,\n","        padding=padding,\n","        strides=strides,\n","    )(inputs)\n","\n","    if activation:\n","        x = LeakyReLU(alpha=0.2)(x)\n","        x = Dropout(0.3)(x)\n","    return x\n","\n","def build_generator(latent_dim):\n","    f = [2**i for i in range(5)][::-1]\n","    filters = 32\n","    output_strides = 16\n","    h_output = IMG_H // output_strides\n","    w_output = IMG_W // output_strides\n","\n","    noise = Input(shape=(latent_dim,), name=\"generator_noise_input\")\n","\n","    x = Dense(f[0] * filters * h_output * w_output, use_bias=False)(noise)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Reshape((h_output, w_output, 16 * filters))(x)\n","\n","    for i in range(1, 5):\n","        x = deconv_block(x,\n","            num_filters=f[i] * filters,\n","            kernel_size=5,\n","            strides=2,\n","            bn=True\n","        )\n","\n","    x = conv_block(x,\n","        num_filters=3,  ## Change this to 1 for grayscale.\n","        kernel_size=5,\n","        strides=1,\n","        activation=False\n","    )\n","    fake_output = Activation(\"tanh\")(x)\n","\n","    return Model(noise, fake_output, name=\"generator\")\n","\n","def build_discriminator():\n","    f = [2**i for i in range(4)]\n","    image_input = Input(shape=(IMG_H, IMG_W, IMG_C))\n","    x = image_input\n","    filters = 64\n","    output_strides = 16\n","    h_output = IMG_H // output_strides\n","    w_output = IMG_W // output_strides\n","\n","    for i in range(0, 4):\n","        x = conv_block(x, num_filters=f[i] * filters, kernel_size=5, strides=2)\n","\n","    x = Flatten()(x)\n","    x = Dense(1)(x)\n","\n","    return Model(image_input, x, name=\"discriminator\")\n","\n","class GAN(Model):\n","    def __init__(self, discriminator, generator, latent_dim):\n","        super(GAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","\n","    def compile(self, d_optimizer, g_optimizer, loss_fn):\n","        super(GAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","\n","    def train_step(self, real_images):\n","        batch_size = tf.shape(real_images)[0]\n","\n","        for _ in range(2):\n","            ## Train the discriminator\n","            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","            generated_images = self.generator(random_latent_vectors)\n","            generated_labels = tf.zeros((batch_size, 1))\n","\n","            with tf.GradientTape() as ftape:\n","                predictions = self.discriminator(generated_images)\n","                d1_loss = self.loss_fn(generated_labels, predictions)\n","            grads = ftape.gradient(d1_loss, self.discriminator.trainable_weights)\n","            self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n","\n","            ## Train the discriminator\n","            labels = tf.ones((batch_size, 1))\n","\n","            with tf.GradientTape() as rtape:\n","                predictions = self.discriminator(real_images)\n","                d2_loss = self.loss_fn(labels, predictions)\n","            grads = rtape.gradient(d2_loss, self.discriminator.trainable_weights)\n","            self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n","\n","        ## Train the generator\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","        misleading_labels = tf.ones((batch_size, 1))\n","\n","        with tf.GradientTape() as gtape:\n","            predictions = self.discriminator(self.generator(random_latent_vectors))\n","            g_loss = self.loss_fn(misleading_labels, predictions)\n","        grads = gtape.gradient(g_loss, self.generator.trainable_weights)\n","        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n","\n","        return {\"d1_loss\": d1_loss, \"d2_loss\": d2_loss, \"g_loss\": g_loss}\n","\n","def save_plot(examples, epoch, n):\n","    examples = (examples + 1) / 2.0\n","    for i in range(n * n):\n","        pyplot.subplot(n, n, i+1)\n","        pyplot.axis(\"off\")\n","        pyplot.imshow(examples[i])  ## pyplot.imshow(np.squeeze(examples[i], axis=-1))\n","    filename = f\"samples/generated_plot_epoch-{epoch+1}.png\"\n","    pyplot.savefig(filename)\n","    pyplot.close()\n","\n","\n","if __name__ == \"__main__\":\n","    ## Hyperparameters\n","    batch_size = 128\n","    latent_dim = 128\n","    num_epochs = 1000\n","    images_path = glob(\"data/*\")\n","\n","    d_model = build_discriminator()\n","    g_model = build_generator(latent_dim)\n","\n","    # d_model.load_weights(\"saved_model/d_model.h5\")\n","    # g_model.load_weights(\"saved_model/g_model.h5\")\n","\n","    d_model.summary()\n","    g_model.summary()\n","\n","    gan = GAN(d_model, g_model, latent_dim)\n","\n","    bce_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1)\n","    d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n","    g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n","    gan.compile(d_optimizer, g_optimizer, bce_loss_fn)\n","\n","    images_dataset = tf_dataset(images_path, batch_size)\n","\n","    for epoch in range(num_epochs):\n","        gan.fit(images_dataset, epochs=1)\n","        g_model.save(\"saved_model/g_model.h5\")\n","        d_model.save(\"saved_model/d_model.h5\")\n","\n","        n_samples = 25\n","        noise = np.random.normal(size=(n_samples, latent_dim))\n","        examples = g_model.predict(noise)\n","        save_plot(examples, epoch, int(np.sqrt(n_samples)))\n","\n","\n","    ##"],"execution_count":null,"outputs":[]}]}