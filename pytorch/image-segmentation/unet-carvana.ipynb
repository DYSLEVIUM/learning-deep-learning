{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n\nimport torchvision\n\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-25T08:48:42.814214Z","iopub.execute_input":"2023-08-25T08:48:42.814589Z","iopub.status.idle":"2023-08-25T08:48:45.176622Z","shell.execute_reply.started":"2023-08-25T08:48:42.814559Z","shell.execute_reply":"2023-08-25T08:48:45.175561Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n        \n    def forward(self, x):\n        return self.conv(x)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.178528Z","iopub.execute_input":"2023-08-25T08:48:45.179277Z","iopub.status.idle":"2023-08-25T08:48:45.187345Z","shell.execute_reply.started":"2023-08-25T08:48:45.179237Z","shell.execute_reply":"2023-08-25T08:48:45.186378Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# class BilinearInterpolationConv(nn.Module):\n#     def __init__(self, in_channels, out_channels, kernel_size):\n#         super(BilinearInterpolationConv, self).__init__()\n        \n#         # Bilinear interpolation layer\n#         self.bilinear = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        \n#         # Convolutional layer\n#         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2)\n\n#     def forward(self, x):\n#         x = self.bilinear(x)\n#         x = self.conv(x)\n#         return x","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.189379Z","iopub.execute_input":"2023-08-25T08:48:45.190186Z","iopub.status.idle":"2023-08-25T08:48:45.204632Z","shell.execute_reply.started":"2023-08-25T08:48:45.190152Z","shell.execute_reply":"2023-08-25T08:48:45.203680Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n        super(UNet, self).__init__()\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # down part of the UNet\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n            \n        # up part of the UNet:\n        for feature in reversed(features):\n        # originally, in the UNet paper, they used bilinear then a conv layer but we will use ConvTranspose2d as it creates artifcats and would be a better approach\n            self.ups.append(\n                # we are doing features*2 as we will be appending the skip connections\n                nn.ConvTranspose2d(\n                    feature*2, feature, kernel_size=2, stride=2\n                ),\n            )\n            self.ups.append(DoubleConv(feature*2, feature))\n            \n        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n        \n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        skip_connections = []\n        \n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n            \n        x = self.bottleneck(x)\n        \n        skip_connections = skip_connections[::-1]\n        \n        # doing two steps for each skip connection\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n            \n            # if the image was not divisible by 16 (max-pool will floor), we cannot just concatenate; in the paper they had used cropping to address this issue, but we can use padding also\n            if x.shape != skip_connection.shape:\n                x = TF.resize(x, size=skip_connection.shape[2:], antialias=True)\n            \n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            \n            x = self.ups[idx + 1](concat_skip)\n            \n        return self.final_conv(x)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.208885Z","iopub.execute_input":"2023-08-25T08:48:45.209201Z","iopub.status.idle":"2023-08-25T08:48:45.223539Z","shell.execute_reply.started":"2023-08-25T08:48:45.209166Z","shell.execute_reply":"2023-08-25T08:48:45.222676Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters etc.\nLEARNING_RATE = 1e-4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 4\nNUM_EPOCHS = 8\nNUM_WORKERS = os.cpu_count()\n# IMAGE_HEIGHT = 160  # 1280 originally\n# IMAGE_WIDTH = 240  # 1918 originally\nIMAGE_HEIGHT = 320  # 1280 originally\nIMAGE_WIDTH = 480  # 1918 originally\nPIN_MEMORY = True\nTRAIN_IMG_DIR = \"/kaggle/working/train\"\nTEST_IMG_DIR = \"/kaggle/working/test\"\nMASK_DIR = \"/kaggle/working/train_masks\"\n\n# clear the GPUs\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.224625Z","iopub.execute_input":"2023-08-25T08:48:45.224958Z","iopub.status.idle":"2023-08-25T08:48:45.299199Z","shell.execute_reply.started":"2023-08-25T08:48:45.224922Z","shell.execute_reply":"2023-08-25T08:48:45.298264Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# !unzip -q /kaggle/input/carvana-image-masking-challenge/train.zip\n# !unzip -q /kaggle/input/carvana-image-masking-challenge/test.zip\n# !unzip -q /kaggle/input/carvana-image-masking-challenge/train_masks.zip","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.300822Z","iopub.execute_input":"2023-08-25T08:48:45.301546Z","iopub.status.idle":"2023-08-25T08:48:45.309942Z","shell.execute_reply.started":"2023-08-25T08:48:45.301512Z","shell.execute_reply":"2023-08-25T08:48:45.309046Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class CarvanaDatasetLoader(Dataset):\n    def __init__(self, image_dir, mask_dir):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.images = os.listdir(image_dir)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.image_dir, self.images[index])\n        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n        mask[mask == 255.0] = 1.0 # we are using a sigmoid in the last activation, built-in with BCEWithLogitsLoss\n\n        return image, mask\n    \nclass CarvanaDataset(Dataset):\n    def __init__(self, subset, transform=None):\n        self.subset = subset\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        image, mask = self.subset[index]\n        \n        # data to augmentations as named arguments\n        if self.transform:\n            augmentations = self.transform(image=image, mask=mask)\n            image = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n\n        return image, mask\n        \n    def __len__(self):\n        return len(self.subset)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.313147Z","iopub.execute_input":"2023-08-25T08:48:45.313445Z","iopub.status.idle":"2023-08-25T08:48:45.327283Z","shell.execute_reply.started":"2023-08-25T08:48:45.313421Z","shell.execute_reply":"2023-08-25T08:48:45.326324Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_transform = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Rotate(limit=35, p=1.0),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0,\n        ),\n        ToTensorV2(),\n    ],\n)\n\nval_transforms = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0,\n        ),\n        ToTensorV2(),\n    ],\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.329811Z","iopub.execute_input":"2023-08-25T08:48:45.330595Z","iopub.status.idle":"2023-08-25T08:48:45.511752Z","shell.execute_reply.started":"2023-08-25T08:48:45.330544Z","shell.execute_reply":"2023-08-25T08:48:45.510609Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"dataset = CarvanaDatasetLoader(TRAIN_IMG_DIR, MASK_DIR)\n\ntrain_set_size = int(len(dataset) * 0.8)\nval_set_size = len(dataset) - train_set_size\n\ntrain_split, val_split = random_split(dataset, [train_set_size, val_set_size])\ntrain_dataset, val_dataset = CarvanaDataset(train_split, transform=train_transform), CarvanaDataset(val_split, transform=val_transforms)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.513448Z","iopub.execute_input":"2023-08-25T08:48:45.513881Z","iopub.status.idle":"2023-08-25T08:48:45.530197Z","shell.execute_reply.started":"2023-08-25T08:48:45.513847Z","shell.execute_reply":"2023-08-25T08:48:45.529252Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# class CarvanaDataset(Dataset):\n#     def __init__(self, image_dir, mask_dir, transform=None):\n#         self.image_dir = image_dir\n#         self.mask_dir = mask_dir\n#         self.images = os.listdir(image_dir)\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.images)\n\n#     def __getitem__(self, index):\n#         img_path = os.path.join(self.image_dir, self.images[index])\n#         mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n#         image = np.array(Image.open(img_path).convert(\"RGB\"))\n#         mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n#         mask[mask == 255.0] = 1.0 # we are using a sigmoid in the last activation, built-in with BCEWithLogitsLoss\n\n#         if self.transform:\n#             augmentations = self.transform(image=image, mask=mask)\n#             image = augmentations[\"image\"]\n#             mask = augmentations[\"mask\"]\n            \n#         return image, mask","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.534951Z","iopub.execute_input":"2023-08-25T08:48:45.535221Z","iopub.status.idle":"2023-08-25T08:48:45.540413Z","shell.execute_reply.started":"2023-08-25T08:48:45.535191Z","shell.execute_reply":"2023-08-25T08:48:45.539360Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# train_dataset, val_dataset = CarvanaDataset(TRAIN_IMG_DIR, MASK_DIR, transform=train_transform), CarvanaDataset(TEST_IMG_DIR, MASK_DIR, transform=val_transforms)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.542368Z","iopub.execute_input":"2023-08-25T08:48:45.542725Z","iopub.status.idle":"2023-08-25T08:48:45.550428Z","shell.execute_reply.started":"2023-08-25T08:48:45.542691Z","shell.execute_reply":"2023-08-25T08:48:45.549482Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.551403Z","iopub.execute_input":"2023-08-25T08:48:45.551694Z","iopub.status.idle":"2023-08-25T08:48:45.565833Z","shell.execute_reply.started":"2023-08-25T08:48:45.551665Z","shell.execute_reply":"2023-08-25T08:48:45.564911Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def train_fn(loader, model, optimizer, loss_fn, scaler):\n    loop = tqdm(loader)\n\n    for batch_idx, (data, targets) in enumerate(loop):\n        data = data.to(device=DEVICE)\n        targets = targets.float().unsqueeze(1).to(device=DEVICE) # adding as extra-dimension to compensate for the batch-size\n\n        # forward\n        with torch.cuda.amp.autocast():\n            predictions = model(data)\n            loss = loss_fn(predictions, targets)\n\n        # backward\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # update tqdm loop\n        loop.set_postfix(loss=loss.item())","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.567418Z","iopub.execute_input":"2023-08-25T08:48:45.567825Z","iopub.status.idle":"2023-08-25T08:48:45.578049Z","shell.execute_reply.started":"2023-08-25T08:48:45.567794Z","shell.execute_reply":"2023-08-25T08:48:45.577092Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def check_accuracy(loader, model, device=\"cuda\"):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    \n    model.eval()\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n            # IoU = Intersection / Union = TP/(TP+FP+FN) = sigma(y*y_hat)/sigma(y_hat+y-y_hat*y) -> soft probabilities version\n            # dice = 2 * Intersection / (Union + Intersection) = (2 * TP) / (2*TP+FN+FP) = 2*sigma(y_hat*y)/sigma(y_hat+y) -> soft probabilities version\n            # dice score or dice coefficient is equal to F1 score; F1 score = harmonic mean of precision and recall;\n            # precision = TP/(TP+FP); recall = TP/(TP+FN)\n            # harmonic mean gives the mean but is a little biased towards the lower side, i.e., it penalizes the worst score between precision and recall, so it tries to balance the both\n            dice_score += (2 * (preds * y).sum()) / (\n                (preds + y).sum() + 1e-8\n            ) # this is a better metric, as giving black pixel will always result in accuracy over 70%, similar to object dectetion, where intersection over union is better\n\n    print(\n        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n    )\n    print(f\"Dice score: {dice_score/len(loader)}\")\n    model.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.579428Z","iopub.execute_input":"2023-08-25T08:48:45.580315Z","iopub.status.idle":"2023-08-25T08:48:45.589796Z","shell.execute_reply.started":"2023-08-25T08:48:45.580290Z","shell.execute_reply":"2023-08-25T08:48:45.588717Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!mkdir \"saved_images\"\n# !rm -rf \"saved_images\"","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:45.592145Z","iopub.execute_input":"2023-08-25T08:48:45.592436Z","iopub.status.idle":"2023-08-25T08:48:46.558522Z","shell.execute_reply.started":"2023-08-25T08:48:45.592412Z","shell.execute_reply":"2023-08-25T08:48:46.557338Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘saved_images’: File exists\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_predictions_as_imgs(\n    loader, model, folder=\"saved_images/\", device=\"cuda\"\n):\n    model.eval()\n    for idx, (x, y) in enumerate(loader):\n        x = x.to(device=device)\n        with torch.no_grad():\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n        torchvision.utils.save_image(\n            preds, f\"{folder}/{idx}_pred.png\"\n        )\n        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n\n    model.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:46.560229Z","iopub.execute_input":"2023-08-25T08:48:46.560976Z","iopub.status.idle":"2023-08-25T08:48:46.569274Z","shell.execute_reply.started":"2023-08-25T08:48:46.560935Z","shell.execute_reply":"2023-08-25T08:48:46.568275Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = UNet(in_channels=3, out_channels=1).to(DEVICE)\nloss_fn = nn.BCEWithLogitsLoss() # change to cross-entropy loss if using for mulit-class classification; this may not be the best loss function as about 70% of the image is background and the model just collapse and it would still be 70%\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# check_accuracy(val_loader, model, device=DEVICE)\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(NUM_EPOCHS):\n    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n\n    # check accuracy\n    check_accuracy(val_loader, model, device=DEVICE)\n\n    # print some examples to a folder\n    if epoch == NUM_EPOCHS - 1:\n        save_predictions_as_imgs(\n            val_loader, model, folder=\"saved_images/\", device=DEVICE\n        )","metadata":{"execution":{"iopub.status.busy":"2023-08-25T08:48:46.570669Z","iopub.execute_input":"2023-08-25T08:48:46.571135Z","iopub.status.idle":"2023-08-25T09:30:53.317476Z","shell.execute_reply.started":"2023-08-25T08:48:46.571104Z","shell.execute_reply":"2023-08-25T09:30:53.316022Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"100%|██████████| 1017/1017 [04:14<00:00,  3.99it/s, loss=0.0657]\n","output_type":"stream"},{"name":"stdout","text":"Got 155222289/156364800 with acc 99.27\nDice score: 0.9830320477485657\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1017/1017 [04:13<00:00,  4.01it/s, loss=0.0275]\n","output_type":"stream"},{"name":"stdout","text":"Got 155416469/156364800 with acc 99.39\nDice score: 0.9859538674354553\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1017/1017 [04:14<00:00,  4.00it/s, loss=0.0177]\n","output_type":"stream"},{"name":"stdout","text":"Got 155435943/156364800 with acc 99.41\nDice score: 0.9861006140708923\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1017/1017 [04:13<00:00,  4.01it/s, loss=0.0147]\n","output_type":"stream"},{"name":"stdout","text":"Got 154356814/156364800 with acc 98.72\nDice score: 0.9703584313392639\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1017/1017 [04:14<00:00,  4.00it/s, loss=0.0156] \n","output_type":"stream"},{"name":"stdout","text":"Got 155768090/156364800 with acc 99.62\nDice score: 0.9910872578620911\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1017/1017 [04:12<00:00,  4.02it/s, loss=0.014]  \n","output_type":"stream"},{"name":"stdout","text":"Got 155835419/156364800 with acc 99.66\nDice score: 0.9920828938484192\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1017/1017 [04:13<00:00,  4.02it/s, loss=0.0107] \n","output_type":"stream"},{"name":"stdout","text":"Got 155820364/156364800 with acc 99.65\nDice score: 0.9918399453163147\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1017/1017 [04:13<00:00,  4.02it/s, loss=0.00676]\n","output_type":"stream"},{"name":"stdout","text":"Got 155759194/156364800 with acc 99.61\nDice score: 0.9909412264823914\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -qr saved.zip /kaggle/working/saved_images","metadata":{"execution":{"iopub.status.busy":"2023-08-25T09:30:53.319578Z","iopub.execute_input":"2023-08-25T09:30:53.322007Z","iopub.status.idle":"2023-08-25T09:30:54.467577Z","shell.execute_reply.started":"2023-08-25T09:30:53.321957Z","shell.execute_reply":"2023-08-25T09:30:54.466287Z"},"trusted":true},"execution_count":18,"outputs":[]}]}
