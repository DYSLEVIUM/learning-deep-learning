{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-25T08:48:42.814589Z","iopub.status.busy":"2023-08-25T08:48:42.814214Z","iopub.status.idle":"2023-08-25T08:48:45.176622Z","shell.execute_reply":"2023-08-25T08:48:45.175561Z","shell.execute_reply.started":"2023-08-25T08:48:42.814559Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import os\n","import numpy as np\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n","\n","import torchvision\n","\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.179277Z","iopub.status.busy":"2023-08-25T08:48:45.178528Z","iopub.status.idle":"2023-08-25T08:48:45.187345Z","shell.execute_reply":"2023-08-25T08:48:45.186378Z","shell.execute_reply.started":"2023-08-25T08:48:45.179237Z"},"trusted":true},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DoubleConv, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","        \n","    def forward(self, x):\n","        return self.conv(x)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.190186Z","iopub.status.busy":"2023-08-25T08:48:45.189379Z","iopub.status.idle":"2023-08-25T08:48:45.204632Z","shell.execute_reply":"2023-08-25T08:48:45.203680Z","shell.execute_reply.started":"2023-08-25T08:48:45.190152Z"},"trusted":true},"outputs":[],"source":["# class BilinearInterpolationConv(nn.Module):\n","#     def __init__(self, in_channels, out_channels, kernel_size):\n","#         super(BilinearInterpolationConv, self).__init__()\n","        \n","#         # Bilinear interpolation layer\n","#         self.bilinear = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        \n","#         # Convolutional layer\n","#         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2)\n","\n","#     def forward(self, x):\n","#         x = self.bilinear(x)\n","#         x = self.conv(x)\n","#         return x"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.209201Z","iopub.status.busy":"2023-08-25T08:48:45.208885Z","iopub.status.idle":"2023-08-25T08:48:45.223539Z","shell.execute_reply":"2023-08-25T08:48:45.222676Z","shell.execute_reply.started":"2023-08-25T08:48:45.209166Z"},"trusted":true},"outputs":[],"source":["class UNet(nn.Module):\n","    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n","        super(UNet, self).__init__()\n","        self.ups = nn.ModuleList()\n","        self.downs = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        \n","        # down part of the UNet\n","        for feature in features:\n","            self.downs.append(DoubleConv(in_channels, feature))\n","            in_channels = feature\n","            \n","        # up part of the UNet:\n","        for feature in reversed(features):\n","        # originally, in the UNet paper, they used bilinear then a conv layer but we will use ConvTranspose2d as it creates artifcats and would be a better approach\n","            self.ups.append(\n","                # we are doing features*2 as we will be appending the skip connections\n","                nn.ConvTranspose2d(\n","                    feature*2, feature, kernel_size=2, stride=2\n","                ),\n","            )\n","            self.ups.append(DoubleConv(feature*2, feature))\n","            \n","        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n","        \n","        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n","        \n","    def forward(self, x):\n","        skip_connections = []\n","        \n","        for down in self.downs:\n","            x = down(x)\n","            skip_connections.append(x)\n","            x = self.pool(x)\n","            \n","        x = self.bottleneck(x)\n","        \n","        skip_connections = skip_connections[::-1]\n","        \n","        # doing two steps for each skip connection\n","        for idx in range(0, len(self.ups), 2):\n","            x = self.ups[idx](x)\n","            skip_connection = skip_connections[idx//2]\n","            \n","            # if the image was not divisible by 16 (max-pool will floor), we cannot just concatenate; in the paper they had used cropping to address this issue, but we can use padding also\n","            if x.shape != skip_connection.shape:\n","                x = TF.resize(x, size=skip_connection.shape[2:], antialias=True)\n","            \n","            concat_skip = torch.cat((skip_connection, x), dim=1)\n","            \n","            x = self.ups[idx + 1](concat_skip)\n","            \n","        return self.final_conv(x)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.224958Z","iopub.status.busy":"2023-08-25T08:48:45.224625Z","iopub.status.idle":"2023-08-25T08:48:45.299199Z","shell.execute_reply":"2023-08-25T08:48:45.298264Z","shell.execute_reply.started":"2023-08-25T08:48:45.224922Z"},"trusted":true},"outputs":[],"source":["# Hyperparameters etc.\n","LEARNING_RATE = 1e-4\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE = 4\n","NUM_EPOCHS = 8\n","NUM_WORKERS = os.cpu_count()\n","# IMAGE_HEIGHT = 160  # 1280 originally\n","# IMAGE_WIDTH = 240  # 1918 originally\n","IMAGE_HEIGHT = 320  # 1280 originally\n","IMAGE_WIDTH = 480  # 1918 originally\n","PIN_MEMORY = True\n","TRAIN_IMG_DIR = \"/kaggle/working/train\"\n","TEST_IMG_DIR = \"/kaggle/working/test\"\n","MASK_DIR = \"/kaggle/working/train_masks\"\n","\n","# clear the GPUs\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.301546Z","iopub.status.busy":"2023-08-25T08:48:45.300822Z","iopub.status.idle":"2023-08-25T08:48:45.309942Z","shell.execute_reply":"2023-08-25T08:48:45.309046Z","shell.execute_reply.started":"2023-08-25T08:48:45.301512Z"},"trusted":true},"outputs":[],"source":["# !unzip -q /kaggle/input/carvana-image-masking-challenge/train.zip\n","# !unzip -q /kaggle/input/carvana-image-masking-challenge/test.zip\n","# !unzip -q /kaggle/input/carvana-image-masking-challenge/train_masks.zip"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.313445Z","iopub.status.busy":"2023-08-25T08:48:45.313147Z","iopub.status.idle":"2023-08-25T08:48:45.327283Z","shell.execute_reply":"2023-08-25T08:48:45.326324Z","shell.execute_reply.started":"2023-08-25T08:48:45.313421Z"},"trusted":true},"outputs":[],"source":["class CarvanaDatasetLoader(Dataset):\n","    def __init__(self, image_dir, mask_dir):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.images = os.listdir(image_dir)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.image_dir, self.images[index])\n","        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n","        mask[mask == 255.0] = 1.0 # we are using a sigmoid in the last activation, built-in with BCEWithLogitsLoss\n","\n","        return image, mask\n","    \n","class CarvanaDataset(Dataset):\n","    def __init__(self, subset, transform=None):\n","        self.subset = subset\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        image, mask = self.subset[index]\n","        \n","        # data to augmentations as named arguments\n","        if self.transform:\n","            augmentations = self.transform(image=image, mask=mask)\n","            image = augmentations[\"image\"]\n","            mask = augmentations[\"mask\"]\n","\n","        return image, mask\n","        \n","    def __len__(self):\n","        return len(self.subset)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.330595Z","iopub.status.busy":"2023-08-25T08:48:45.329811Z","iopub.status.idle":"2023-08-25T08:48:45.511752Z","shell.execute_reply":"2023-08-25T08:48:45.510609Z","shell.execute_reply.started":"2023-08-25T08:48:45.330544Z"},"trusted":true},"outputs":[],"source":["train_transform = A.Compose(\n","    [\n","        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","        A.Rotate(limit=35, p=1.0),\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.1),\n","        A.Normalize(\n","            mean=[0.0, 0.0, 0.0],\n","            std=[1.0, 1.0, 1.0],\n","            max_pixel_value=255.0,\n","        ),\n","        ToTensorV2(),\n","    ],\n",")\n","\n","val_transforms = A.Compose(\n","    [\n","        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","        A.Normalize(\n","            mean=[0.0, 0.0, 0.0],\n","            std=[1.0, 1.0, 1.0],\n","            max_pixel_value=255.0,\n","        ),\n","        ToTensorV2(),\n","    ],\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.513881Z","iopub.status.busy":"2023-08-25T08:48:45.513448Z","iopub.status.idle":"2023-08-25T08:48:45.530197Z","shell.execute_reply":"2023-08-25T08:48:45.529252Z","shell.execute_reply.started":"2023-08-25T08:48:45.513847Z"},"trusted":true},"outputs":[],"source":["dataset = CarvanaDatasetLoader(TRAIN_IMG_DIR, MASK_DIR)\n","\n","train_set_size = int(len(dataset) * 0.8)\n","val_set_size = len(dataset) - train_set_size\n","\n","train_split, val_split = random_split(dataset, [train_set_size, val_set_size])\n","train_dataset, val_dataset = CarvanaDataset(train_split, transform=train_transform), CarvanaDataset(val_split, transform=val_transforms)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.535221Z","iopub.status.busy":"2023-08-25T08:48:45.534951Z","iopub.status.idle":"2023-08-25T08:48:45.540413Z","shell.execute_reply":"2023-08-25T08:48:45.539360Z","shell.execute_reply.started":"2023-08-25T08:48:45.535191Z"},"trusted":true},"outputs":[],"source":["# class CarvanaDataset(Dataset):\n","#     def __init__(self, image_dir, mask_dir, transform=None):\n","#         self.image_dir = image_dir\n","#         self.mask_dir = mask_dir\n","#         self.images = os.listdir(image_dir)\n","#         self.transform = transform\n","\n","#     def __len__(self):\n","#         return len(self.images)\n","\n","#     def __getitem__(self, index):\n","#         img_path = os.path.join(self.image_dir, self.images[index])\n","#         mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n","#         image = np.array(Image.open(img_path).convert(\"RGB\"))\n","#         mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n","#         mask[mask == 255.0] = 1.0 # we are using a sigmoid in the last activation, built-in with BCEWithLogitsLoss\n","\n","#         if self.transform:\n","#             augmentations = self.transform(image=image, mask=mask)\n","#             image = augmentations[\"image\"]\n","#             mask = augmentations[\"mask\"]\n","            \n","#         return image, mask"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.542725Z","iopub.status.busy":"2023-08-25T08:48:45.542368Z","iopub.status.idle":"2023-08-25T08:48:45.550428Z","shell.execute_reply":"2023-08-25T08:48:45.549482Z","shell.execute_reply.started":"2023-08-25T08:48:45.542691Z"},"trusted":true},"outputs":[],"source":["# train_dataset, val_dataset = CarvanaDataset(TRAIN_IMG_DIR, MASK_DIR, transform=train_transform), CarvanaDataset(TEST_IMG_DIR, MASK_DIR, transform=val_transforms)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.551694Z","iopub.status.busy":"2023-08-25T08:48:45.551403Z","iopub.status.idle":"2023-08-25T08:48:45.565833Z","shell.execute_reply":"2023-08-25T08:48:45.564911Z","shell.execute_reply.started":"2023-08-25T08:48:45.551665Z"},"trusted":true},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.567825Z","iopub.status.busy":"2023-08-25T08:48:45.567418Z","iopub.status.idle":"2023-08-25T08:48:45.578049Z","shell.execute_reply":"2023-08-25T08:48:45.577092Z","shell.execute_reply.started":"2023-08-25T08:48:45.567794Z"},"trusted":true},"outputs":[],"source":["def train_fn(loader, model, optimizer, loss_fn, scaler):\n","    loop = tqdm(loader)\n","\n","    for batch_idx, (data, targets) in enumerate(loop):\n","        data = data.to(device=DEVICE)\n","        targets = targets.float().unsqueeze(1).to(device=DEVICE) # adding as extra-dimension to compensate for the batch-size\n","\n","        # forward\n","        with torch.cuda.amp.autocast():\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","\n","        # Both approaches are valid for the standard use case, i.e. if you do not want to accumulate gradients for multiple iterations.\n","        # You can thus call optimizer.zero_grad() everywhere in the loop but not between the loss.backward() and optimizer.step() operation.\n","        \n","        # backward\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # update tqdm loop\n","        loop.set_postfix(loss=loss.item())"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.580315Z","iopub.status.busy":"2023-08-25T08:48:45.579428Z","iopub.status.idle":"2023-08-25T08:48:45.589796Z","shell.execute_reply":"2023-08-25T08:48:45.588717Z","shell.execute_reply.started":"2023-08-25T08:48:45.580290Z"},"trusted":true},"outputs":[],"source":["def check_accuracy(loader, model, device=\"cuda\"):\n","    num_correct = 0\n","    num_pixels = 0\n","    dice_score = 0\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device)\n","            y = y.to(device).unsqueeze(1)\n","            preds = torch.sigmoid(model(x))\n","            preds = (preds > 0.5).float()\n","            num_correct += (preds == y).sum()\n","            num_pixels += torch.numel(preds)\n","            # IoU = Intersection / Union = TP/(TP+FP+FN) = sigma(y*y_hat)/sigma(y_hat+y-y_hat*y) -> soft probabilities version\n","            # dice = 2 * Intersection / (Union + Intersection) = (2 * TP) / (2*TP+FN+FP) = 2*sigma(y_hat*y)/sigma(y_hat+y) -> soft probabilities version\n","            # dice score or dice coefficient is equal to F1 score; F1 score = harmonic mean of precision and recall;\n","            # precision = TP/(TP+FP); recall = TP/(TP+FN)\n","            # harmonic mean gives the mean but is a little biased towards the lower side, i.e., it penalizes the worst score between precision and recall, so it tries to balance the both\n","            dice_score += (2 * (preds * y).sum()) / (\n","                (preds + y).sum() + 1e-8\n","            ) # this is a better metric, as giving black pixel will always result in accuracy over 70%, similar to object dectetion, where intersection over union is better\n","\n","    print(\n","        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n","    )\n","    print(f\"Dice score: {dice_score/len(loader)}\")\n","    model.train()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:45.592436Z","iopub.status.busy":"2023-08-25T08:48:45.592145Z","iopub.status.idle":"2023-08-25T08:48:46.558522Z","shell.execute_reply":"2023-08-25T08:48:46.557338Z","shell.execute_reply.started":"2023-08-25T08:48:45.592412Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘saved_images’: File exists\n"]}],"source":["!mkdir \"saved_images\"\n","# !rm -rf \"saved_images\""]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:46.560976Z","iopub.status.busy":"2023-08-25T08:48:46.560229Z","iopub.status.idle":"2023-08-25T08:48:46.569274Z","shell.execute_reply":"2023-08-25T08:48:46.568275Z","shell.execute_reply.started":"2023-08-25T08:48:46.560935Z"},"trusted":true},"outputs":[],"source":["def save_predictions_as_imgs(\n","    loader, model, folder=\"saved_images/\", device=\"cuda\"\n","):\n","    model.eval()\n","    for idx, (x, y) in enumerate(loader):\n","        x = x.to(device=device)\n","        with torch.no_grad():\n","            preds = torch.sigmoid(model(x))\n","            preds = (preds > 0.5).float()\n","        torchvision.utils.save_image(\n","            preds, f\"{folder}/{idx}_pred.png\"\n","        )\n","        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n","\n","    model.train()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T08:48:46.571135Z","iopub.status.busy":"2023-08-25T08:48:46.570669Z","iopub.status.idle":"2023-08-25T09:30:53.317476Z","shell.execute_reply":"2023-08-25T09:30:53.316022Z","shell.execute_reply.started":"2023-08-25T08:48:46.571104Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1017/1017 [04:14<00:00,  3.99it/s, loss=0.0657]\n"]},{"name":"stdout","output_type":"stream","text":["Got 155222289/156364800 with acc 99.27\n","Dice score: 0.9830320477485657\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1017/1017 [04:13<00:00,  4.01it/s, loss=0.0275]\n"]},{"name":"stdout","output_type":"stream","text":["Got 155416469/156364800 with acc 99.39\n","Dice score: 0.9859538674354553\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1017/1017 [04:14<00:00,  4.00it/s, loss=0.0177]\n"]},{"name":"stdout","output_type":"stream","text":["Got 155435943/156364800 with acc 99.41\n","Dice score: 0.9861006140708923\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1017/1017 [04:13<00:00,  4.01it/s, loss=0.0147]\n"]},{"name":"stdout","output_type":"stream","text":["Got 154356814/156364800 with acc 98.72\n","Dice score: 0.9703584313392639\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1017/1017 [04:14<00:00,  4.00it/s, loss=0.0156] \n"]},{"name":"stdout","output_type":"stream","text":["Got 155768090/156364800 with acc 99.62\n","Dice score: 0.9910872578620911\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1017/1017 [04:12<00:00,  4.02it/s, loss=0.014]  \n"]},{"name":"stdout","output_type":"stream","text":["Got 155835419/156364800 with acc 99.66\n","Dice score: 0.9920828938484192\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1017/1017 [04:13<00:00,  4.02it/s, loss=0.0107] \n"]},{"name":"stdout","output_type":"stream","text":["Got 155820364/156364800 with acc 99.65\n","Dice score: 0.9918399453163147\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1017/1017 [04:13<00:00,  4.02it/s, loss=0.00676]\n"]},{"name":"stdout","output_type":"stream","text":["Got 155759194/156364800 with acc 99.61\n","Dice score: 0.9909412264823914\n"]}],"source":["model = UNet(in_channels=3, out_channels=1).to(DEVICE)\n","loss_fn = nn.BCEWithLogitsLoss() # change to cross-entropy loss if using for mulit-class classification; this may not be the best loss function as about 70% of the image is background and the model just collapse and it would still be 70%\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# check_accuracy(val_loader, model, device=DEVICE)\n","scaler = torch.cuda.amp.GradScaler()\n","\n","for epoch in range(NUM_EPOCHS):\n","    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n","\n","    # check accuracy\n","    check_accuracy(val_loader, model, device=DEVICE)\n","\n","    # print some examples to a folder\n","    if epoch == NUM_EPOCHS - 1:\n","        save_predictions_as_imgs(\n","            val_loader, model, folder=\"saved_images/\", device=DEVICE\n","        )"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T09:30:53.322007Z","iopub.status.busy":"2023-08-25T09:30:53.319578Z","iopub.status.idle":"2023-08-25T09:30:54.467577Z","shell.execute_reply":"2023-08-25T09:30:54.466287Z","shell.execute_reply.started":"2023-08-25T09:30:53.321957Z"},"trusted":true},"outputs":[],"source":["!zip -qr saved.zip /kaggle/working/saved_images"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
